import json
import pickle
import sys
from pathlib import Path

import pandas as pd

main_dir_path = str(Path(__file__).parent.parent.parent)
if main_dir_path not in sys.path:
    sys.path.append(main_dir_path)

from poisoned_rag_defence.logger import logger


class Experiment:
    """A class to load up and store experiment results.

    Lazily loads saved files as required, and throws an error pointing to which script to run if the files don't exist yet.
    """

    EXPERIMENT_DIR = Path("./results/experiments")
    CACHE_DIR = Path("./.cache")

    def __init__(self, name):
        self.name = name
        self.results_dir = Experiment.EXPERIMENT_DIR / name

        # Created by running generate_poisonedrag_cache.py for this dataset
        self._qrels = None
        self._corpus = None
        self._test_cases = None

        # These are all loaded in lazily and will throw errors if they haven't been generated yet
        self._config = None

        # These are generated by running run_retriever.py for this experiment
        self._question_df = None
        self._context_df = None

        # These are generated by running generate_openai_embeddings.py for this experiment
        self._question_embedding_df = None
        self._extended_corpus_df = None
        self._corpus_embedding_df = None

    @property
    def config(self) -> dict[str, any]:
        if self._config is None:
            try:
                with open(self.results_dir / "config.json", "r") as fd:
                    return json.load(fd)
            except FileNotFoundError as e:
                raise Exception(
                    f"Unable to find config for experiment {self.name}. Have you run 'initialise_experiment_set.py' for that experiment?"
                ) from e
        else:
            return self._config

    def save_config(self, config: dict[str, any]):
        self.results_dir.mkdir(parents=True, exist_ok=True)
        with open(self.results_dir / "config.json", "w") as fd:
            json.dump(config, fd, indent=2)

    # Generated by running the retriever
    def _load_question_context_dfs(self):
        try:
            self._question_df = pd.read_pickle(self.results_dir / "questions.p")
            self._context_df = pd.read_pickle(self.results_dir / "context.p")
        except FileNotFoundError as e:
            raise Exception(
                f"Unable to find question and context dfs for experiment {self.name}. Have you run 'run_retriever.py' for that experiment?"
            ) from e

    @property
    def question_df(self) -> dict[str, any]:
        if self._question_df is None:
            self._load_question_context_dfs()
        return self._question_df

    @property
    def context_df(self) -> dict[str, any]:
        if self._context_df is None:
            self._load_question_context_dfs()
        return self._context_df

    def _get_openai_embeddings(self):
        try:
            self._corpus_embedding_df = pd.read_pickle(
                Experiment.CACHE_DIR
                / f"openai_corpus_embedding_cache-{self.config['dataset']}.p"
            )
            self._question_embedding_df = pd.read_pickle(
                Experiment.CACHE_DIR
                / f"openai_question_embedding_cache-{self.config['dataset']}.p"
            )
            self._extended_corpus_df = pd.read_pickle(
                Experiment.CACHE_DIR / f"extended_corpus-{self.config['dataset']}.p"
            )
        except FileNotFoundError as e:
            raise Exception(
                f"Not able to find cache file for dataset {self.config['dataset']}. Have you run 'generate_openai_embeddings.py'?"
            ) from e

    @property
    def corpus_embedding_df(self) -> dict[str, any]:
        if self._corpus_embedding_df is None:
            self._get_openai_embeddings()
        return self._corpus_embedding_df

    @property
    def question_embedding_df(self) -> dict[str, any]:
        if self._question_embedding_df is None:
            self._get_openai_embeddings()
        return self._question_embedding_df

    @property
    def extended_corpus_df(self) -> dict[str, any]:
        if self._extended_corpus_df is None:
            self._get_openai_embeddings()
        return self._extended_corpus_df

    def _load_beir_cache(self):
        try:
            with open(
                Experiment.CACHE_DIR / f"beir_cache-{self.config['dataset']}.p",
                "rb",
            ) as fd:
                self._corpus, self._qrels = pickle.load(fd)
        except FileNotFoundError as e:
            raise Exception(
                "Not able to find cache file. Have you run 'generate_poisonedrag_cache.py'?"
            ) from e

    def _load_poisonedrag_cache(self):
        try:
            with open(
                Experiment.CACHE_DIR / f"poisonedrag_cache-{self.config['dataset']}.p",
                "rb",
            ) as fd:
                self._test_cases, _ = pickle.load(fd)
        except FileNotFoundError as e:
            raise Exception(
                "Not able to find cache file. Have you run 'generate_poisonedrag_cache.py'?"
            ) from e

    @property
    def test_cases(self) -> list[dict[str, any]]:
        if self._test_cases is None:
            self._load_poisonedrag_cache()
        return self._test_cases

    @property
    def corpus(self) -> list[dict[str, any]]:
        if self._corpus is None:
            self._load_beir_cache()
        return self._corpus

    @property
    def qrels(self) -> list[dict[str, any]]:
        if self._qrels is None:
            self._load_beir_cache()
        return self._qrels

    def save_df(self, df: pd.DataFrame, save_name: str):
        save_file = self.results_dir / f"{save_name}.p"
        df.to_pickle(save_file)
        logger.info(f"Dataframe saved to {save_file}")
